{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import lief\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import configparser\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb09b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "f = 'feature_extraction'\n",
    "\n",
    "full_dos = config.get(f,'full_dos')\n",
    "extend_dos = config.get(f,'extend_dos')\n",
    "content_shift = config.get(f,'content_shift')\n",
    "fgs = config.get(f,'fgsm')\n",
    "gamma = config.get(f,'gamma')\n",
    "gamma_subset = config.get(f, 'gamma_subset_ad')\n",
    "c_full_dos = config.get(f,'c_full_dos')\n",
    "c_extend_dos = config.get(f,'c_extend_dos')\n",
    "c_content_shift = config.get(f,'c_content_shift')\n",
    "c_fgs = config.get(f,'c_fgsm')\n",
    "c_gamma = config.get(f,'c_gamma')\n",
    "c_gamma_subset = config.get(f, 'c_gamma_subset')\n",
    "f_output = config.get(f, 'f_output')\n",
    "\n",
    "\n",
    "if os.path.exists(f_output):\n",
    "    shutil.rmtree(f_output)\n",
    "os.makedirs(f_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50742e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/python\n",
    "''' Extracts some basic features from PE files. Many of the features\n",
    "implemented have been used in previously published works. For more information,\n",
    "check out the following resources:\n",
    "* Schultz, et al., 2001: http://128.59.14.66/sites/default/files/binaryeval-ieeesp01.pdf\n",
    "* Kolter and Maloof, 2006: http://www.jmlr.org/papers/volume7/kolter06a/kolter06a.pdf\n",
    "* Shafiq et al., 2009: https://www.researchgate.net/profile/Fauzan_Mirza/publication/242084613_A_Framework_for_Efficient_Mining_of_Structural_Information_to_Detect_Zero-Day_Malicious_Portable_Executables/links/0c96052e191668c3d5000000.pdf\n",
    "* Raman, 2012: http://2012.infosecsouthwest.com/files/speaker_materials/ISSW2012_Selecting_Features_to_Classify_Malware.pdf\n",
    "* Saxe and Berlin, 2015: https://arxiv.org/pdf/1508.03096.pdf\n",
    "It may be useful to do feature selection to reduce this set of features to a meaningful set\n",
    "for your modeling problem.\n",
    "'''\n",
    "\n",
    "LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split('.')\n",
    "LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or ( int(LIEF_MAJOR)==0 and int(LIEF_MINOR) >= 10 )\n",
    "LIEF_HAS_SIGNATURE = int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 11)\n",
    "\n",
    "\n",
    "class FeatureType(object):\n",
    "    ''' Base class from which each feature type may inherit '''\n",
    "\n",
    "    name = ''\n",
    "    dim = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, self.dim)\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        ''' Generate a JSON-able representation of the file '''\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        ''' Generate a feature vector from the raw features '''\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def feature_vector(self, bytez, lief_binary):\n",
    "        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n",
    "        if there are significant speedups to be gained from combining the two functions. '''\n",
    "        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n",
    "\n",
    "\n",
    "class ByteHistogram(FeatureType):\n",
    "    ''' Byte histogram (count + non-normalized) over the entire binary file '''\n",
    "\n",
    "    name = 'histogram'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n",
    "        return counts.tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class ByteEntropyHistogram(FeatureType):\n",
    "    ''' 2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).\n",
    "    This roughly approximates the joint probability of byte value and local entropy.\n",
    "    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.\n",
    "    '''\n",
    "\n",
    "    name = 'byteentropy'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self, step=1024, window=2048):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "\n",
    "    def _entropy_bin_counts(self, block):\n",
    "        # coarse histogram, 16 bytes per bin\n",
    "        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram\n",
    "        p = c.astype(np.float32) / self.window\n",
    "        wh = np.where(c)[0]\n",
    "        H = np.sum(-p[wh] * np.log2(\n",
    "            p[wh])) * 2  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)\n",
    "\n",
    "        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)\n",
    "        if Hbin == 16:  # handle entropy = 8.0 bits\n",
    "            Hbin = 15\n",
    "\n",
    "        return Hbin, c\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = np.zeros((16, 16), dtype=np.int)\n",
    "        a = np.frombuffer(bytez, dtype=np.uint8)\n",
    "        if a.shape[0] < self.window:\n",
    "            Hbin, c = self._entropy_bin_counts(a)\n",
    "            output[Hbin, :] += c\n",
    "        else:\n",
    "            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n",
    "            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n",
    "            strides = a.strides + (a.strides[-1],)\n",
    "            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n",
    "\n",
    "            # from the blocks, compute histogram\n",
    "            for block in blocks:\n",
    "                Hbin, c = self._entropy_bin_counts(block)\n",
    "                output[Hbin, :] += c\n",
    "\n",
    "        return output.flatten().tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class SectionInfo(FeatureType):\n",
    "    ''' Information about section names, sizes and entropy.  Uses hashing trick\n",
    "    to summarize all this section info into a feature vector.\n",
    "    '''\n",
    "\n",
    "    name = 'section'\n",
    "    dim = 5 + 50 + 50 + 50 + 50 + 50\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _properties(s):\n",
    "        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\"entry\": \"\", \"sections\": []}\n",
    "\n",
    "        # properties of entry point, or if invalid, the first executable section\n",
    "\n",
    "        try:\n",
    "            if int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 12):\n",
    "                section = lief_binary.section_from_rva(lief_binary.entrypoint - lief_binary.imagebase)\n",
    "                if section is None:\n",
    "                    raise lief.not_found\n",
    "                entry_section = section.name\n",
    "            else: # lief < 0.12\n",
    "                entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n",
    "        except lief.not_found:\n",
    "                # bad entry point, let's find the first executable section\n",
    "                entry_section = \"\"\n",
    "                for s in lief_binary.sections:\n",
    "                    if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n",
    "                        entry_section = s.name\n",
    "                        break\n",
    "\n",
    "        raw_obj = {\"entry\": entry_section}\n",
    "        raw_obj[\"sections\"] = [{\n",
    "            'name': s.name,\n",
    "            'size': s.size,\n",
    "            'entropy': s.entropy,\n",
    "            'vsize': s.virtual_size,\n",
    "            'props': self._properties(s)\n",
    "        } for s in lief_binary.sections]\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        sections = raw_obj['sections']\n",
    "        general = [\n",
    "            len(sections),  # total number of sections\n",
    "            # number of sections with zero size\n",
    "            sum(1 for s in sections if s['size'] == 0),\n",
    "            # number of sections with an empty name\n",
    "            sum(1 for s in sections if s['name'] == \"\"),\n",
    "            # number of RX\n",
    "            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
    "            # number of W\n",
    "            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
    "        ]\n",
    "        # gross characteristics of each section\n",
    "        section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "        section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
    "        section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([raw_obj['entry']]).toarray()[0]\n",
    "        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
    "        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "\n",
    "        return np.hstack([\n",
    "            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n",
    "            characteristics_hashed\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ImportsInfo(FeatureType):\n",
    "    ''' Information about imported libraries and functions from the\n",
    "    import address table.  Note that the total number of imported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'imports'\n",
    "    dim = 1280\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        imports = {}\n",
    "        if lief_binary is None:\n",
    "            return imports\n",
    "\n",
    "        for lib in lief_binary.imports:\n",
    "            if lib.name not in imports:\n",
    "                imports[lib.name] = []  # libraries can be duplicated in listing, extend instead of overwrite\n",
    "\n",
    "            # Clipping assumes there are diminishing returns on the discriminatory power of imported functions\n",
    "            #  beyond the first 10000 characters, and this will help limit the dataset size\n",
    "            for entry in lib.entries:\n",
    "                if entry.is_ordinal:\n",
    "                    imports[lib.name].append(\"ordinal\" + str(entry.ordinal))\n",
    "                else:\n",
    "                    imports[lib.name].append(entry.name[:10000])\n",
    "\n",
    "        return imports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        # unique libraries\n",
    "        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n",
    "\n",
    "        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n",
    "        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n",
    "        imports_hashed = FeatureHasher(1024, input_type=\"string\").transform([imports]).toarray()[0]\n",
    "\n",
    "        # Two separate elements: libraries (alone) and fully-qualified names of imported functions\n",
    "        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ExportsInfo(FeatureType):\n",
    "    ''' Information about exported functions. Note that the total number of exported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'exports'\n",
    "    dim = 128\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return []\n",
    "\n",
    "        # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond\n",
    "        #  the first 10000 characters, and this will help limit the dataset size\n",
    "        if LIEF_EXPORT_OBJECT:\n",
    "            # export is an object with .name attribute (0.10.0 and later)\n",
    "            clipped_exports = [export.name[:10000] for export in lief_binary.exported_functions]\n",
    "        else:\n",
    "            # export is a string (LIEF 0.9.0 and earlier)\n",
    "            clipped_exports = [export[:10000] for export in lief_binary.exported_functions]\n",
    "\n",
    "\n",
    "        return clipped_exports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n",
    "        return exports_hashed.astype(np.float32)\n",
    "\n",
    "\n",
    "class GeneralFileInfo(FeatureType):\n",
    "    ''' General information about the file '''\n",
    "\n",
    "    name = 'general'\n",
    "    dim = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\n",
    "                'size': len(bytez),\n",
    "                'vsize': 0,\n",
    "                'has_debug': 0,\n",
    "                'exports': 0,\n",
    "                'imports': 0,\n",
    "                'has_relocations': 0,\n",
    "                'has_resources': 0,\n",
    "                'has_signature': 0,\n",
    "                'has_tls': 0,\n",
    "                'symbols': 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'size': len(bytez),\n",
    "            'vsize': lief_binary.virtual_size,\n",
    "            'has_debug': int(lief_binary.has_debug),\n",
    "            'exports': len(lief_binary.exported_functions),\n",
    "            'imports': len(lief_binary.imported_functions),\n",
    "            'has_relocations': int(lief_binary.has_relocations),\n",
    "            'has_resources': int(lief_binary.has_resources),\n",
    "            'has_signature': int(lief_binary.has_signatures) if LIEF_HAS_SIGNATURE else int(lief_binary.has_signature),\n",
    "            'has_tls': int(lief_binary.has_tls),\n",
    "            'symbols': len(lief_binary.symbols),\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.asarray([\n",
    "            raw_obj['size'], raw_obj['vsize'], raw_obj['has_debug'], raw_obj['exports'], raw_obj['imports'],\n",
    "            raw_obj['has_relocations'], raw_obj['has_resources'], raw_obj['has_signature'], raw_obj['has_tls'],\n",
    "            raw_obj['symbols']\n",
    "        ],\n",
    "                          dtype=np.float32)\n",
    "\n",
    "\n",
    "class HeaderFileInfo(FeatureType):\n",
    "    ''' Machine, architecure, OS, linker and other information extracted from header '''\n",
    "\n",
    "    name = 'header'\n",
    "    dim = 62\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        raw_obj = {}\n",
    "        raw_obj['coff'] = {'timestamp': 0, 'machine': \"\", 'characteristics': []}\n",
    "        raw_obj['optional'] = {\n",
    "            'subsystem': \"\",\n",
    "            'dll_characteristics': [],\n",
    "            'magic': \"\",\n",
    "            'major_image_version': 0,\n",
    "            'minor_image_version': 0,\n",
    "            'major_linker_version': 0,\n",
    "            'minor_linker_version': 0,\n",
    "            'major_operating_system_version': 0,\n",
    "            'minor_operating_system_version': 0,\n",
    "            'major_subsystem_version': 0,\n",
    "            'minor_subsystem_version': 0,\n",
    "            'sizeof_code': 0,\n",
    "            'sizeof_headers': 0,\n",
    "            'sizeof_heap_commit': 0\n",
    "        }\n",
    "        if lief_binary is None:\n",
    "            return raw_obj\n",
    "\n",
    "        raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps\n",
    "        raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]\n",
    "        raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]\n",
    "        raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]\n",
    "        raw_obj['optional']['dll_characteristics'] = [\n",
    "            str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists\n",
    "        ]\n",
    "        raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]\n",
    "        raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version\n",
    "        raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version\n",
    "        raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version\n",
    "        raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version\n",
    "        raw_obj['optional'][\n",
    "            'major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version\n",
    "        raw_obj['optional'][\n",
    "            'minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version\n",
    "        raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version\n",
    "        raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version\n",
    "        raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code\n",
    "        raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers\n",
    "        raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.hstack([\n",
    "            raw_obj['coff']['timestamp'],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['coff']['machine']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['coff']['characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['subsystem']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['optional']['dll_characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['magic']]]).toarray()[0],\n",
    "            raw_obj['optional']['major_image_version'],\n",
    "            raw_obj['optional']['minor_image_version'],\n",
    "            raw_obj['optional']['major_linker_version'],\n",
    "            raw_obj['optional']['minor_linker_version'],\n",
    "            raw_obj['optional']['major_operating_system_version'],\n",
    "            raw_obj['optional']['minor_operating_system_version'],\n",
    "            raw_obj['optional']['major_subsystem_version'],\n",
    "            raw_obj['optional']['minor_subsystem_version'],\n",
    "            raw_obj['optional']['sizeof_code'],\n",
    "            raw_obj['optional']['sizeof_headers'],\n",
    "            raw_obj['optional']['sizeof_heap_commit'],\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class StringExtractor(FeatureType):\n",
    "    ''' Extracts strings from raw byte stream '''\n",
    "\n",
    "    name = 'strings'\n",
    "    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # all consecutive runs of 0x20 - 0x7f that are 5+ characters\n",
    "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
    "        # occurances of the string 'C:\\'.  Not actually extracting the path\n",
    "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
    "        # occurances of http:// or https://.  Not actually extracting the URLs\n",
    "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
    "        # occurances of the string prefix HKEY_.  No actually extracting registry names\n",
    "        self._registry = re.compile(b'HKEY_')\n",
    "        # crude evidence of an MZ header (dropper?) somewhere in the byte stream\n",
    "        self._mz = re.compile(b'MZ')\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        allstrings = self._allstrings.findall(bytez)\n",
    "        if allstrings:\n",
    "            # statistics about strings:\n",
    "            string_lengths = [len(s) for s in allstrings]\n",
    "            avlength = sum(string_lengths) / len(string_lengths)\n",
    "            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
    "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
    "            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n",
    "            # distribution of characters in printable strings\n",
    "            csum = c.sum()\n",
    "            p = c.astype(np.float32) / csum\n",
    "            wh = np.where(c)[0]\n",
    "            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n",
    "        else:\n",
    "            avlength = 0\n",
    "            c = np.zeros((96,), dtype=np.float32)\n",
    "            H = 0\n",
    "            csum = 0\n",
    "\n",
    "        return {\n",
    "            'numstrings': len(allstrings),\n",
    "            'avlength': avlength,\n",
    "            'printabledist': c.tolist(),  # store non-normalized histogram\n",
    "            'printables': int(csum),\n",
    "            'entropy': float(H),\n",
    "            'paths': len(self._paths.findall(bytez)),\n",
    "            'urls': len(self._urls.findall(bytez)),\n",
    "            'registry': len(self._registry.findall(bytez)),\n",
    "            'MZ': len(self._mz.findall(bytez))\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n",
    "        return np.hstack([\n",
    "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
    "            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n",
    "            raw_obj['registry'], raw_obj['MZ']\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class DataDirectories(FeatureType):\n",
    "    ''' Extracts size and virtual address of the first 15 data directories '''\n",
    "\n",
    "    name = 'datadirectories'\n",
    "    dim = 15 * 2\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self._name_order = [\n",
    "            \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n",
    "            \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n",
    "            \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n",
    "        ]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = []\n",
    "        if lief_binary is None:\n",
    "            return output\n",
    "\n",
    "        for data_directory in lief_binary.data_directories:\n",
    "            output.append({\n",
    "                \"name\": str(data_directory.type).replace(\"DATA_DIRECTORY.\", \"\"),\n",
    "                \"size\": data_directory.size,\n",
    "                \"virtual_address\": data_directory.rva\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        features = np.zeros(2 * len(self._name_order), dtype=np.float32)\n",
    "        for i in range(len(self._name_order)):\n",
    "            if i < len(raw_obj):\n",
    "                features[2 * i] = raw_obj[i][\"size\"]\n",
    "                features[2 * i + 1] = raw_obj[i][\"virtual_address\"]\n",
    "        return features\n",
    "\n",
    "\n",
    "class PEFeatureExtractor(object):\n",
    "    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n",
    "\n",
    "    def __init__(self, feature_version=2, print_feature_warning=True, features_file=''):\n",
    "        self.features = []\n",
    "        features = {\n",
    "                    'ByteHistogram': ByteHistogram(),\n",
    "                    'ByteEntropyHistogram': ByteEntropyHistogram(),\n",
    "                    'StringExtractor': StringExtractor(),\n",
    "                    'GeneralFileInfo': GeneralFileInfo(),\n",
    "                    'HeaderFileInfo': HeaderFileInfo(),\n",
    "                    'SectionInfo': SectionInfo(),\n",
    "                    'ImportsInfo': ImportsInfo(),\n",
    "                    'ExportsInfo': ExportsInfo()\n",
    "            }\n",
    "\n",
    "        if os.path.exists(features_file):\n",
    "            with open(features_file, encoding='utf8') as f:\n",
    "                x = json.load(f)\n",
    "                self.features = [features[feature] for feature in x['features'] if feature in features]\n",
    "        else:\n",
    "            self.features = list(features.values())\n",
    "\n",
    "        if feature_version == 1:\n",
    "            if not lief.__version__.startswith(\"0.8.3\"):\n",
    "                if print_feature_warning:\n",
    "                    print(f\"WARNING: EMBER feature version 1 were computed using lief version 0.8.3-18d5b75\")\n",
    "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
    "                    print(f\"WARNING:   in the feature calculations.\")\n",
    "        elif feature_version == 2:\n",
    "            self.features.append(DataDirectories())\n",
    "            if not lief.__version__.startswith(\"0.9.0\"):\n",
    "                if print_feature_warning:\n",
    "                    print(f\"WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\")\n",
    "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
    "                    print(f\"WARNING:   in the feature calculations.\")\n",
    "        else:\n",
    "            raise Exception(f\"EMBER feature version must be 1 or 2. Not {feature_version}\")\n",
    "        self.dim = sum([fe.dim for fe in self.features])\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n",
    "                       RuntimeError)\n",
    "        try:\n",
    "            lief_binary = lief.PE.parse(list(bytez))\n",
    "        except lief_errors as e:\n",
    "            print(\"lief error: \", str(e))\n",
    "            lief_binary = None\n",
    "        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n",
    "            raise\n",
    "\n",
    "        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n",
    "        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n",
    "        return features\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n",
    "        return np.hstack(feature_vectors).astype(np.float32)\n",
    "\n",
    "    def feature_vector(self, bytez):\n",
    "        return self.process_raw_features(self.raw_features(bytez))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e31792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_name():\n",
    "    for h in range(1, 257):\n",
    "        header.append(f\"hist_{h}\")\n",
    "    for b in range(257, 513):\n",
    "        header.append(f\"byte_{b}\")\n",
    "    for s in range(513, 617):\n",
    "        header.append(f\"string_{s}\")\n",
    "    for g in range(617, 627):\n",
    "        header.append(f\"gen_{g}\")\n",
    "    for h in range(627, 689):\n",
    "        header.append(f\"head_{h}\")\n",
    "    for sec in range(689, 944):\n",
    "        header.append(f\"section_{sec}\")\n",
    "    for imp in range(944, 2224):\n",
    "        header.append(f\"imports_{imp}\")\n",
    "    for exp in range(2224, 2352):\n",
    "        header.append(f\"exports_{exp}\")\n",
    "    for d in range(2352, 2382):\n",
    "        header.append(f\"directories_{d}\")\n",
    "def extract_dir_name(path):\n",
    "    path = path.rstrip('/')\n",
    "    last_dir_name = os.path.basename(path)\n",
    "    return last_dir_name\n",
    "\n",
    "def f_extract(malicious_PE_dir, adversarial_PE_dir):\n",
    "    # Extract the last directory names for both paths\n",
    "    c_file_name = extract_dir_name(malicious_PE_dir)\n",
    "    a_file_name = extract_dir_name(adversarial_PE_dir)\n",
    "    #extract file names in sorted form\n",
    "    malicious_files = os.listdir(malicious_PE_dir)\n",
    "    adversarial_files = os.listdir(adversarial_PE_dir)\n",
    "    malicious_files.sort()\n",
    "    adversarial_files.sort()\n",
    "    malicious_featurs = []\n",
    "    adversarial_features = []\n",
    "    feature_extractor = PEFeatureExtractor()\n",
    "    for malicious_file, adversarial_file in zip(malicious_files, adversarial_files):\n",
    "        malicious_file_path = os.path.join(malicious_PE_dir, malicious_file)\n",
    "        adversarial_file_path = os.path.join(adversarial_PE_dir, adversarial_file)\n",
    "        print(\"m\", malicious_file_path)\n",
    "        print(\"a\", adversarial_file_path)\n",
    "        with open( malicious_file_path, \"rb\") as file_handle:\n",
    "            malicious_file_code = file_handle.read()\n",
    "       \n",
    "        with open(adversarial_file_path, \"rb\") as file_handle:\n",
    "            adversarial_file_code = file_handle.read() \n",
    "        malicious_vectorized = feature_extractor.feature_vector(malicious_file_code)\n",
    "        malicious_featurs.append(malicious_vectorized)\n",
    "        adversarial_vectorized = feature_extractor.feature_vector(adversarial_file_code)\n",
    "        adversarial_features.append(adversarial_vectorized)\n",
    "\n",
    "    with open(os.path.join(f_output, f\"{c_file_name}.csv\"), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for ve in malicious_featurs:\n",
    "            writer.writerow(ve) \n",
    "    \n",
    "    with open(os.path.join(f_output, f\"{a_file_name}.csv\"), 'w', newline='') as d:\n",
    "        writer = csv.writer(d)\n",
    "        writer.writerow(header)\n",
    "        for ae in adversarial_features:\n",
    "            writer.writerow(ae) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "f_name()\n",
    "path_pairs = [(c_full_dos, full_dos), (c_extend_dos, extend_dos), (c_content_shift, content_shift ), (c_fgs, fgs),(c_gamma, gamma), (c_gamma_subset, gamma_subset) ]\n",
    "for path1, path2 in path_pairs:\n",
    "    f_extract(path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6f165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
