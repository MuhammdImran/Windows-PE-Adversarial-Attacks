{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f489c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import shutil\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import csv\n",
    "import configparser\n",
    "import re\n",
    "import lief\n",
    "import hashlib\n",
    "import json\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "np.random.seed=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "at = 'adv_training'\n",
    "at_type = config.getint(at, 'At_type')\n",
    "original_samples = config.get(at, 'original_samples')\n",
    "malware = config.get(at, 'malware_files')\n",
    "goodware = config.get(at, 'goodware_files')\n",
    "full_dos = config.get(at, 'full_dos')\n",
    "extend_dos = config.get(at, 'extend_dos')\n",
    "content_shift = config.get(at, 'content_shift')\n",
    "fgsm = config.get(at, 'fgsm')\n",
    "gamma = config.get(at, 'gamma')\n",
    "#output folders path\n",
    "output = config.get(at, 'output')\n",
    "fdos_output = config.get(at, 'fdos_output')\n",
    "fdos_models = config.get(at, 'fdos_models')\n",
    "fdos_results = config.get(at, 'fdos_results')\n",
    "extend_output = config.get(at, 'extend_output')\n",
    "extend_models = config.get(at, 'extend_models')\n",
    "extend_results = config.get(at, 'extend_results')\n",
    "content_output = config.get(at, 'content_output')\n",
    "content_models = config.get(at, 'content_models')\n",
    "content_results = config.get(at, 'content_results')\n",
    "fgs_output = config.get(at, 'fgs_output')\n",
    "fgs_models = config.get(at, 'fgs_models')\n",
    "fgs_results = config.get(at, 'fgs_results')\n",
    "gamma_output = config.get(at, 'gamma_output')\n",
    "gamma_models = config.get(at, 'gamma_models')\n",
    "gamma_results = config.get(at, 'gamma_results')\n",
    "\n",
    "if at_type == 1:\n",
    "    adversarial = full_dos \n",
    "    output_folder = fdos_output\n",
    "    model_folder = fdos_models\n",
    "    results_folder = fdos_results\n",
    "    if os.path.exists(fdos_output):\n",
    "        shutil.rmtree(fdos_output)\n",
    "    os.makedirs(fdos_output, exist_ok=True)\n",
    "    if os.path.exists(fdos_models):\n",
    "        shutil.rmtree(fdos_models)\n",
    "    os.makedirs(fdos_models, exist_ok=True)\n",
    "    if os.path.exists(fdos_results):\n",
    "        shutil.rmtree(fdos_results)\n",
    "    os.makedirs(fdos_results, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "if at_type ==2:\n",
    "    adversarial = extend_dos \n",
    "    output_folder = extend_output\n",
    "    model_folder = extend_models\n",
    "    results_folder = extend_results\n",
    "    if os.path.exists(extend_output):\n",
    "        shutil.rmtree(extend_output)\n",
    "    os.makedirs(extend_output, exist_ok=True)\n",
    "    if os.path.exists(extend_models):\n",
    "        shutil.rmtree(extend_models)\n",
    "    os.makedirs(extend_models, exist_ok=True)\n",
    "    if os.path.exists(extend_results):\n",
    "        shutil.rmtree(extend_results)\n",
    "    os.makedirs(extend_results, exist_ok=True)\n",
    "if at_type ==3:\n",
    "    adversarial = content_shift \n",
    "    output_folder = content_output\n",
    "    model_folder = content_models\n",
    "    results_folder = content_results\n",
    "    \n",
    "    if os.path.exists(content_output):\n",
    "        shutil.rmtree(content_output)\n",
    "    os.makedirs(content_output, exist_ok=True)\n",
    "    if os.path.exists(content_models):\n",
    "        shutil.rmtree(content_models)\n",
    "    os.makedirs(content_models, exist_ok=True)\n",
    "    if os.path.exists(content_results):\n",
    "        shutil.rmtree(content_results)\n",
    "    os.makedirs(content_results, exist_ok=True)\n",
    "if at_type ==4:\n",
    "    adversarial = fgsm \n",
    "    output_folder = fgs_output\n",
    "    model_folder = fgs_models\n",
    "    results_folder = fgs_results\n",
    "    \n",
    "    if os.path.exists(fgs_output):\n",
    "        shutil.rmtree(fgs_output)\n",
    "    os.makedirs(fgs_output, exist_ok=True)\n",
    "    if os.path.exists(fgs_models):\n",
    "        shutil.rmtree(fgs_models)\n",
    "    os.makedirs(fgs_models, exist_ok=True)\n",
    "    if os.path.exists(fgs_results):\n",
    "        shutil.rmtree(fgs_results)\n",
    "    os.makedirs(fgs_results, exist_ok=True)\n",
    "if at_type ==5:\n",
    "    adversarial = gamma \n",
    "    output_folder = gamma_output\n",
    "    model_folder = gamma_models\n",
    "    results_folder = gamma_results\n",
    "\n",
    "    if os.path.exists(gamma_output):\n",
    "        shutil.rmtree(gamma_output)\n",
    "    os.makedirs(gamma_output, exist_ok=True)\n",
    "    if os.path.exists(gamma_models):\n",
    "        shutil.rmtree(gamma_models)\n",
    "    os.makedirs(gamma_models, exist_ok=True)\n",
    "    if os.path.exists(gamma_results):\n",
    "        shutil.rmtree(gamma_results)\n",
    "    os.makedirs(gamma_results, exist_ok=True)\n",
    "    \n",
    "\n",
    "if os.path.exists(original_samples):\n",
    "    shutil.rmtree(original_samples)\n",
    "os.makedirs(original_samples, exist_ok=True)\n",
    "    \n",
    "n_splits = 3\n",
    "folds_folders = [\"fold1\", \"fold2\", \"fold3\"]\n",
    "csv_files_and_destinations = {\n",
    "    \"fold1_data.csv\": \"fold1\",\n",
    "    \"fold2_data.csv\": \"fold2\",\n",
    "    \"fold3_data.csv\": \"fold3\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcf3a2",
   "metadata": {},
   "source": [
    "### copy binary files from malware, goodware folder and paste it into original samples folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(source_folder, original_samples):\n",
    "    files = os.listdir(source_folder)\n",
    "    for file_name in files:\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        destination_path = os.path.join(original_samples, file_name)\n",
    "        shutil.copy(source_path, destination_path)\n",
    "    \n",
    "        \n",
    "# Copy files from malicious and goodware folders to the original_samples folder\n",
    "copy_files(malware, original_samples)\n",
    "copy_files(goodware, original_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38add9f8",
   "metadata": {},
   "source": [
    "### Extracting files names and labels from malware and goodware samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract file names and labels form benign and maliocus samples\n",
    "malware_files = os.listdir(malware)\n",
    "goodware_files = os.listdir(goodware)\n",
    "all_files = malware_files  + goodware_files\n",
    "labels = [1] * len(malware_files ) + [0] * len(goodware_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49c14f",
   "metadata": {},
   "source": [
    "##### apply stratifiedkfold to split the extratced files names and labels into three folds and saved in folder fold1, fold2 and fold3  respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851075c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_samples = [[] for _ in range(n_splits)]\n",
    "\n",
    "# Collect sample names and labels for each fold without moving files\n",
    "for fold, (_, test_index) in enumerate(skf.split(all_files, labels), 1):\n",
    "    fold_samples[fold - 1] = [(all_files[i], labels[i]) for i in test_index]\n",
    "\n",
    "    # Create a folder for the current fold within the main output directory\n",
    "    fold_folder = os.path.join(output_folder, f'fold{fold}')\n",
    "    \n",
    "    # Create a new folder for the current fold\n",
    "    os.makedirs(fold_folder)\n",
    "\n",
    "    # Create a DataFrame for the current fold\n",
    "    fold_df = pd.DataFrame(fold_samples[fold - 1], columns=['file_names', 'labels'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file in the current fold folder\n",
    "    csv_filename = f'fold{fold}_data.csv'\n",
    "    csv_path = os.path.join(fold_folder, csv_filename)\n",
    "    fold_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    samples_with_label_1 = sum(1 for _, label in fold_samples[fold - 1] if label == 1)\n",
    "    samples_with_label_0 = sum(1 for _, label in fold_samples[fold - 1] if label == 0)\n",
    "\n",
    "    print(f\"Fold {fold}: {len(test_index)} samples with label 1: {samples_with_label_1}, samples with label 0: {samples_with_label_0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755266a",
   "metadata": {},
   "source": [
    "#### copy binary files from original sample and paste into folder fold1, fold2, fold3 by reading files names from corresponding folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c50de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move binary files into fold1, fold2, fold3\n",
    "# Loop through CSV files and move files based on CSV content\n",
    "for csv_file, corresponding_folder in csv_files_and_destinations.items():\n",
    "    fold_file_path = os.path.join(output_folder, corresponding_folder, csv_file)\n",
    "\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(fold_file_path)\n",
    "    fold_file_names = df['file_names'].tolist()\n",
    "\n",
    "    # Create a new subfolder with a modified name within the existing subfolder\n",
    "    new_folder_name = f\"{corresponding_folder}_binary_files\"\n",
    "    full_destination_path = os.path.join(output_folder, corresponding_folder, new_folder_name)\n",
    "    os.makedirs(full_destination_path, exist_ok=True)\n",
    "\n",
    "    # List all files in the source folder\n",
    "    original_files = os.listdir(original_samples)\n",
    "\n",
    "    # Move files based on CSV content\n",
    "    for file_name in fold_file_names:\n",
    "        source_path = os.path.join(original_samples, file_name)\n",
    "        destination_path = os.path.join(full_destination_path, file_name)\n",
    "        shutil.copy(source_path, destination_path)\n",
    "        print(f\"Moved {file_name} to {destination_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa980385",
   "metadata": {},
   "source": [
    "#####  original binary files from fold1, fold2, fold3 copied and paste into modified_fold1, modified_fold2, modified_fold3 folders respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of folders inside subfolders fold1, fold2, fold3\n",
    "binary_files_folders = [\"fold1_binary_files\", \"fold2_binary_files\", \"fold3_binary_files\"]\n",
    "# Create modified folders and subfolders if they don't exist\n",
    "for folds_folder, binary_files_folder in zip(folds_folders, binary_files_folders):\n",
    "    modified_folder = f\"modified_{folds_folder}\"\n",
    "    modified_folder_binary = f\"{modified_folder}_binary_files\"\n",
    "    \n",
    "    #full_subfolder_path = os.path.join(output_folder, folds_folder)\n",
    "    modified_folder_path = os.path.join(output_folder, modified_folder)\n",
    "    modified_folder_binary_path = os.path.join(modified_folder_path, modified_folder_binary)\n",
    "    \n",
    "    os.makedirs(modified_folder_binary_path, exist_ok=True)\n",
    "\n",
    "    source_folder = os.path.join(output_folder,folds_folder,binary_files_folder)\n",
    "    destination_folder = modified_folder_binary_path\n",
    "\n",
    "    # List all files in the original folder\n",
    "    files_to_copy = os.listdir(source_folder)\n",
    "\n",
    "    # Copy files to the modified folder binary\n",
    "    for file_name in files_to_copy:\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        destination_path = os.path.join(destination_folder, file_name)\n",
    "        shutil.copy(source_path, destination_path)\n",
    "        print(f\"Copied {file_name} to {destination_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97caf7e",
   "metadata": {},
   "source": [
    "##### Binary files in modified folder replaced with the corresponding adversarial files if same files exists in both folders otherwise keep original in modified folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of modified folders\n",
    "modified_folders = [\"modified_fold1\", \"modified_fold2\", \"modified_fold3\"]\n",
    "# Iterate through each modified folder\n",
    "for modified_folder_name in modified_folders:\n",
    "    modified_folder_path = os.path.join(output_folder, modified_folder_name)\n",
    "    modified_binary_folder_name = f\"{modified_folder_name}_binary_files\"\n",
    "    modified_binary_folder_path = os.path.join(modified_folder_path, modified_binary_folder_name)\n",
    "\n",
    "    # List files in the modified binary folder\n",
    "    modified_binary_files = os.listdir(modified_binary_folder_path)\n",
    "\n",
    "    for modified_file in modified_binary_files:\n",
    "        # Check if the file with the same name exists in the \"adversarial\" folder\n",
    "        if modified_file in os.listdir(adversarial):\n",
    "            modified_file_path = os.path.join(modified_binary_folder_path, modified_file)\n",
    "            adversarial_file_path = os.path.join(adversarial, modified_file)\n",
    "            \n",
    "            print(\"modified_file_path\", modified_file_path)\n",
    "            print(\"adversarial_file_path\", adversarial_file_path)\n",
    "            \n",
    "            shutil.copy2(adversarial_file_path, modified_file_path)\n",
    "            print(f\"Replaced {modified_file} with the file from the 'adversarial' folder.\")\n",
    "        else:\n",
    "            print(f\"No replacement found for {modified_file} in the 'adversarial' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5a2f5",
   "metadata": {},
   "source": [
    "###### read files name from folds and check if same binary files exists in adversarial folder, then copy from adversarial and paste to new folders adv_fold1, adv_fold2 and adv_fold3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each fold folder\n",
    "for csv_file,corresponding_folder in csv_files_and_destinations.items():\n",
    "    csv_file_path = os.path.join(output_folder, corresponding_folder, csv_file)\n",
    "\n",
    "    adv_folder = f\"adv_{corresponding_folder}\"\n",
    "    adv_folder_binary = f\"{adv_folder}_binary_files\"\n",
    "    \n",
    "    #full_subfolder_path = os.path.join(output_folder, folds_folder)\n",
    "    adv_folder_path = os.path.join(output_folder, adv_folder)\n",
    "    adv_files_path = os.path.join(adv_folder_path, adv_folder_binary)\n",
    "    os.makedirs(adv_files_path, exist_ok=True)\n",
    "    # read csv files names\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    file_names = df['file_names'].tolist()\n",
    "    for fold_file in file_names:\n",
    "        # Check if the file with the same name exists in the \"adversarial\" folder\n",
    "        if fold_file in os.listdir(adversarial):\n",
    "            source_path = os.path.join(adversarial, fold_file)\n",
    "\n",
    "            # Move binary files from adversarial folder to adv_fold1, adv_fold2, adv_fold3\n",
    "            destination_path = os.path.join(adv_files_path, fold_file)\n",
    "            shutil.copy2(source_path, destination_path)\n",
    "\n",
    "            print(f\"Moved {fold_file} from 'adversarial' folder to 'adv_{corresponding_folder}'\")\n",
    "        else:\n",
    "            print(f\"No matching file found for {adv_folder} in the 'adversarial' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed033aba",
   "metadata": {},
   "source": [
    "### code to extarct features from windows PE files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/python\n",
    "''' Extracts some basic features from PE files. Many of the features\n",
    "implemented have been used in previously published works. For more information,\n",
    "check out the following resources:\n",
    "* Schultz, et al., 2001: http://128.59.14.66/sites/default/files/binaryeval-ieeesp01.pdf\n",
    "* Kolter and Maloof, 2006: http://www.jmlr.org/papers/volume7/kolter06a/kolter06a.pdf\n",
    "* Shafiq et al., 2009: https://www.researchgate.net/profile/Fauzan_Mirza/publication/242084613_A_Framework_for_Efficient_Mining_of_Structural_Information_to_Detect_Zero-Day_Malicious_Portable_Executables/links/0c96052e191668c3d5000000.pdf\n",
    "* Raman, 2012: http://2012.infosecsouthwest.com/files/speaker_materials/ISSW2012_Selecting_Features_to_Classify_Malware.pdf\n",
    "* Saxe and Berlin, 2015: https://arxiv.org/pdf/1508.03096.pdf\n",
    "It may be useful to do feature selection to reduce this set of features to a meaningful set\n",
    "for your modeling problem.\n",
    "'''\n",
    "\n",
    "LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split('.')\n",
    "LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or ( int(LIEF_MAJOR)==0 and int(LIEF_MINOR) >= 10 )\n",
    "LIEF_HAS_SIGNATURE = int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 11)\n",
    "\n",
    "\n",
    "class FeatureType(object):\n",
    "    ''' Base class from which each feature type may inherit '''\n",
    "\n",
    "    name = ''\n",
    "    dim = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, self.dim)\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        ''' Generate a JSON-able representation of the file '''\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        ''' Generate a feature vector from the raw features '''\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def feature_vector(self, bytez, lief_binary):\n",
    "        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n",
    "        if there are significant speedups to be gained from combining the two functions. '''\n",
    "        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n",
    "\n",
    "\n",
    "class ByteHistogram(FeatureType):\n",
    "    ''' Byte histogram (count + non-normalized) over the entire binary file '''\n",
    "\n",
    "    name = 'histogram'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n",
    "        return counts.tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class ByteEntropyHistogram(FeatureType):\n",
    "    ''' 2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).\n",
    "    This roughly approximates the joint probability of byte value and local entropy.\n",
    "    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.\n",
    "    '''\n",
    "\n",
    "    name = 'byteentropy'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self, step=1024, window=2048):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "\n",
    "    def _entropy_bin_counts(self, block):\n",
    "        # coarse histogram, 16 bytes per bin\n",
    "        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram\n",
    "        p = c.astype(np.float32) / self.window\n",
    "        wh = np.where(c)[0]\n",
    "        H = np.sum(-p[wh] * np.log2(\n",
    "            p[wh])) * 2  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)\n",
    "\n",
    "        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)\n",
    "        if Hbin == 16:  # handle entropy = 8.0 bits\n",
    "            Hbin = 15\n",
    "\n",
    "        return Hbin, c\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = np.zeros((16, 16), dtype=np.int)\n",
    "        a = np.frombuffer(bytez, dtype=np.uint8)\n",
    "        if a.shape[0] < self.window:\n",
    "            Hbin, c = self._entropy_bin_counts(a)\n",
    "            output[Hbin, :] += c\n",
    "        else:\n",
    "            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n",
    "            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n",
    "            strides = a.strides + (a.strides[-1],)\n",
    "            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n",
    "\n",
    "            # from the blocks, compute histogram\n",
    "            for block in blocks:\n",
    "                Hbin, c = self._entropy_bin_counts(block)\n",
    "                output[Hbin, :] += c\n",
    "\n",
    "        return output.flatten().tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class SectionInfo(FeatureType):\n",
    "    ''' Information about section names, sizes and entropy.  Uses hashing trick\n",
    "    to summarize all this section info into a feature vector.\n",
    "    '''\n",
    "\n",
    "    name = 'section'\n",
    "    dim = 5 + 50 + 50 + 50 + 50 + 50\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _properties(s):\n",
    "        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\"entry\": \"\", \"sections\": []}\n",
    "\n",
    "        # properties of entry point, or if invalid, the first executable section\n",
    "\n",
    "        try:\n",
    "            if int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 12):\n",
    "                section = lief_binary.section_from_rva(lief_binary.entrypoint - lief_binary.imagebase)\n",
    "                if section is None:\n",
    "                    raise lief.not_found\n",
    "                entry_section = section.name\n",
    "            else: # lief < 0.12\n",
    "                entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n",
    "        except lief.not_found:\n",
    "                # bad entry point, let's find the first executable section\n",
    "                entry_section = \"\"\n",
    "                for s in lief_binary.sections:\n",
    "                    if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n",
    "                        entry_section = s.name\n",
    "                        break\n",
    "\n",
    "        raw_obj = {\"entry\": entry_section}\n",
    "        raw_obj[\"sections\"] = [{\n",
    "            'name': s.name,\n",
    "            'size': s.size,\n",
    "            'entropy': s.entropy,\n",
    "            'vsize': s.virtual_size,\n",
    "            'props': self._properties(s)\n",
    "        } for s in lief_binary.sections]\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        sections = raw_obj['sections']\n",
    "        general = [\n",
    "            len(sections),  # total number of sections\n",
    "            # number of sections with zero size\n",
    "            sum(1 for s in sections if s['size'] == 0),\n",
    "            # number of sections with an empty name\n",
    "            sum(1 for s in sections if s['name'] == \"\"),\n",
    "            # number of RX\n",
    "            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
    "            # number of W\n",
    "            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
    "        ]\n",
    "        # gross characteristics of each section\n",
    "        section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "        section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
    "        section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([raw_obj['entry']]).toarray()[0]\n",
    "        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
    "        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "\n",
    "        return np.hstack([\n",
    "            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n",
    "            characteristics_hashed\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ImportsInfo(FeatureType):\n",
    "    ''' Information about imported libraries and functions from the\n",
    "    import address table.  Note that the total number of imported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'imports'\n",
    "    dim = 1280\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        imports = {}\n",
    "        if lief_binary is None:\n",
    "            return imports\n",
    "\n",
    "        for lib in lief_binary.imports:\n",
    "            if lib.name not in imports:\n",
    "                imports[lib.name] = []  # libraries can be duplicated in listing, extend instead of overwrite\n",
    "\n",
    "            # Clipping assumes there are diminishing returns on the discriminatory power of imported functions\n",
    "            #  beyond the first 10000 characters, and this will help limit the dataset size\n",
    "            for entry in lib.entries:\n",
    "                if entry.is_ordinal:\n",
    "                    imports[lib.name].append(\"ordinal\" + str(entry.ordinal))\n",
    "                else:\n",
    "                    imports[lib.name].append(entry.name[:10000])\n",
    "\n",
    "        return imports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        # unique libraries\n",
    "        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n",
    "\n",
    "        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n",
    "        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n",
    "        imports_hashed = FeatureHasher(1024, input_type=\"string\").transform([imports]).toarray()[0]\n",
    "\n",
    "        # Two separate elements: libraries (alone) and fully-qualified names of imported functions\n",
    "        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ExportsInfo(FeatureType):\n",
    "    ''' Information about exported functions. Note that the total number of exported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'exports'\n",
    "    dim = 128\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return []\n",
    "\n",
    "        # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond\n",
    "        #  the first 10000 characters, and this will help limit the dataset size\n",
    "        if LIEF_EXPORT_OBJECT:\n",
    "            # export is an object with .name attribute (0.10.0 and later)\n",
    "            clipped_exports = [export.name[:10000] for export in lief_binary.exported_functions]\n",
    "        else:\n",
    "            # export is a string (LIEF 0.9.0 and earlier)\n",
    "            clipped_exports = [export[:10000] for export in lief_binary.exported_functions]\n",
    "\n",
    "\n",
    "        return clipped_exports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n",
    "        return exports_hashed.astype(np.float32)\n",
    "\n",
    "\n",
    "class GeneralFileInfo(FeatureType):\n",
    "    ''' General information about the file '''\n",
    "\n",
    "    name = 'general'\n",
    "    dim = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\n",
    "                'size': len(bytez),\n",
    "                'vsize': 0,\n",
    "                'has_debug': 0,\n",
    "                'exports': 0,\n",
    "                'imports': 0,\n",
    "                'has_relocations': 0,\n",
    "                'has_resources': 0,\n",
    "                'has_signature': 0,\n",
    "                'has_tls': 0,\n",
    "                'symbols': 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'size': len(bytez),\n",
    "            'vsize': lief_binary.virtual_size,\n",
    "            'has_debug': int(lief_binary.has_debug),\n",
    "            'exports': len(lief_binary.exported_functions),\n",
    "            'imports': len(lief_binary.imported_functions),\n",
    "            'has_relocations': int(lief_binary.has_relocations),\n",
    "            'has_resources': int(lief_binary.has_resources),\n",
    "            'has_signature': int(lief_binary.has_signatures) if LIEF_HAS_SIGNATURE else int(lief_binary.has_signature),\n",
    "            'has_tls': int(lief_binary.has_tls),\n",
    "            'symbols': len(lief_binary.symbols),\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.asarray([\n",
    "            raw_obj['size'], raw_obj['vsize'], raw_obj['has_debug'], raw_obj['exports'], raw_obj['imports'],\n",
    "            raw_obj['has_relocations'], raw_obj['has_resources'], raw_obj['has_signature'], raw_obj['has_tls'],\n",
    "            raw_obj['symbols']\n",
    "        ],\n",
    "                          dtype=np.float32)\n",
    "\n",
    "\n",
    "class HeaderFileInfo(FeatureType):\n",
    "    ''' Machine, architecure, OS, linker and other information extracted from header '''\n",
    "\n",
    "    name = 'header'\n",
    "    dim = 62\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        raw_obj = {}\n",
    "        raw_obj['coff'] = {'timestamp': 0, 'machine': \"\", 'characteristics': []}\n",
    "        raw_obj['optional'] = {\n",
    "            'subsystem': \"\",\n",
    "            'dll_characteristics': [],\n",
    "            'magic': \"\",\n",
    "            'major_image_version': 0,\n",
    "            'minor_image_version': 0,\n",
    "            'major_linker_version': 0,\n",
    "            'minor_linker_version': 0,\n",
    "            'major_operating_system_version': 0,\n",
    "            'minor_operating_system_version': 0,\n",
    "            'major_subsystem_version': 0,\n",
    "            'minor_subsystem_version': 0,\n",
    "            'sizeof_code': 0,\n",
    "            'sizeof_headers': 0,\n",
    "            'sizeof_heap_commit': 0\n",
    "        }\n",
    "        if lief_binary is None:\n",
    "            return raw_obj\n",
    "\n",
    "        raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps\n",
    "        raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]\n",
    "        raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]\n",
    "        raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]\n",
    "        raw_obj['optional']['dll_characteristics'] = [\n",
    "            str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists\n",
    "        ]\n",
    "        raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]\n",
    "        raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version\n",
    "        raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version\n",
    "        raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version\n",
    "        raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version\n",
    "        raw_obj['optional'][\n",
    "            'major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version\n",
    "        raw_obj['optional'][\n",
    "            'minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version\n",
    "        raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version\n",
    "        raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version\n",
    "        raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code\n",
    "        raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers\n",
    "        raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.hstack([\n",
    "            raw_obj['coff']['timestamp'],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['coff']['machine']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['coff']['characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['subsystem']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['optional']['dll_characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['magic']]]).toarray()[0],\n",
    "            raw_obj['optional']['major_image_version'],\n",
    "            raw_obj['optional']['minor_image_version'],\n",
    "            raw_obj['optional']['major_linker_version'],\n",
    "            raw_obj['optional']['minor_linker_version'],\n",
    "            raw_obj['optional']['major_operating_system_version'],\n",
    "            raw_obj['optional']['minor_operating_system_version'],\n",
    "            raw_obj['optional']['major_subsystem_version'],\n",
    "            raw_obj['optional']['minor_subsystem_version'],\n",
    "            raw_obj['optional']['sizeof_code'],\n",
    "            raw_obj['optional']['sizeof_headers'],\n",
    "            raw_obj['optional']['sizeof_heap_commit'],\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class StringExtractor(FeatureType):\n",
    "    ''' Extracts strings from raw byte stream '''\n",
    "\n",
    "    name = 'strings'\n",
    "    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # all consecutive runs of 0x20 - 0x7f that are 5+ characters\n",
    "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
    "        # occurances of the string 'C:\\'.  Not actually extracting the path\n",
    "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
    "        # occurances of http:// or https://.  Not actually extracting the URLs\n",
    "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
    "        # occurances of the string prefix HKEY_.  No actually extracting registry names\n",
    "        self._registry = re.compile(b'HKEY_')\n",
    "        # crude evidence of an MZ header (dropper?) somewhere in the byte stream\n",
    "        self._mz = re.compile(b'MZ')\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        allstrings = self._allstrings.findall(bytez)\n",
    "        if allstrings:\n",
    "            # statistics about strings:\n",
    "            string_lengths = [len(s) for s in allstrings]\n",
    "            avlength = sum(string_lengths) / len(string_lengths)\n",
    "            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
    "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
    "            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n",
    "            # distribution of characters in printable strings\n",
    "            csum = c.sum()\n",
    "            p = c.astype(np.float32) / csum\n",
    "            wh = np.where(c)[0]\n",
    "            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n",
    "        else:\n",
    "            avlength = 0\n",
    "            c = np.zeros((96,), dtype=np.float32)\n",
    "            H = 0\n",
    "            csum = 0\n",
    "\n",
    "        return {\n",
    "            'numstrings': len(allstrings),\n",
    "            'avlength': avlength,\n",
    "            'printabledist': c.tolist(),  # store non-normalized histogram\n",
    "            'printables': int(csum),\n",
    "            'entropy': float(H),\n",
    "            'paths': len(self._paths.findall(bytez)),\n",
    "            'urls': len(self._urls.findall(bytez)),\n",
    "            'registry': len(self._registry.findall(bytez)),\n",
    "            'MZ': len(self._mz.findall(bytez))\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n",
    "        return np.hstack([\n",
    "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
    "            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n",
    "            raw_obj['registry'], raw_obj['MZ']\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class DataDirectories(FeatureType):\n",
    "    ''' Extracts size and virtual address of the first 15 data directories '''\n",
    "\n",
    "    name = 'datadirectories'\n",
    "    dim = 15 * 2\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self._name_order = [\n",
    "            \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n",
    "            \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n",
    "            \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n",
    "        ]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = []\n",
    "        if lief_binary is None:\n",
    "            return output\n",
    "\n",
    "        for data_directory in lief_binary.data_directories:\n",
    "            output.append({\n",
    "                \"name\": str(data_directory.type).replace(\"DATA_DIRECTORY.\", \"\"),\n",
    "                \"size\": data_directory.size,\n",
    "                \"virtual_address\": data_directory.rva\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        features = np.zeros(2 * len(self._name_order), dtype=np.float32)\n",
    "        for i in range(len(self._name_order)):\n",
    "            if i < len(raw_obj):\n",
    "                features[2 * i] = raw_obj[i][\"size\"]\n",
    "                features[2 * i + 1] = raw_obj[i][\"virtual_address\"]\n",
    "        return features\n",
    "\n",
    "\n",
    "class PEFeatureExtractor(object):\n",
    "    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n",
    "\n",
    "    def __init__(self, feature_version=2, print_feature_warning=True, features_file=''):\n",
    "        self.features = []\n",
    "        features = {\n",
    "                    'ByteHistogram': ByteHistogram(),\n",
    "                    'ByteEntropyHistogram': ByteEntropyHistogram(),\n",
    "                    'StringExtractor': StringExtractor(),\n",
    "                    'GeneralFileInfo': GeneralFileInfo(),\n",
    "                    'HeaderFileInfo': HeaderFileInfo(),\n",
    "                    'SectionInfo': SectionInfo(),\n",
    "                    'ImportsInfo': ImportsInfo(),\n",
    "                    'ExportsInfo': ExportsInfo()\n",
    "            }\n",
    "\n",
    "        if os.path.exists(features_file):\n",
    "            with open(features_file, encoding='utf8') as f:\n",
    "                x = json.load(f)\n",
    "                self.features = [features[feature] for feature in x['features'] if feature in features]\n",
    "        else:\n",
    "            self.features = list(features.values())\n",
    "\n",
    "        if feature_version == 1:\n",
    "            if not lief.__version__.startswith(\"0.8.3\"):\n",
    "                if print_feature_warning:\n",
    "                    print(f\"WARNING: EMBER feature version 1 were computed using lief version 0.8.3-18d5b75\")\n",
    "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
    "                    print(f\"WARNING:   in the feature calculations.\")\n",
    "        elif feature_version == 2:\n",
    "            self.features.append(DataDirectories())\n",
    "            if not lief.__version__.startswith(\"0.9.0\"):\n",
    "                if print_feature_warning:\n",
    "                    print(f\"WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\")\n",
    "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
    "                    print(f\"WARNING:   in the feature calculations.\")\n",
    "        else:\n",
    "            raise Exception(f\"EMBER feature version must be 1 or 2. Not {feature_version}\")\n",
    "        self.dim = sum([fe.dim for fe in self.features])\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n",
    "                       RuntimeError)\n",
    "        try:\n",
    "            lief_binary = lief.PE.parse(list(bytez))\n",
    "        except lief_errors as e:\n",
    "            #print(\"lief error: \", str(e))\n",
    "            lief_binary = None\n",
    "        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n",
    "            raise\n",
    "\n",
    "        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n",
    "        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n",
    "        return features\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n",
    "        return np.hstack(feature_vectors).astype(np.float32)\n",
    "\n",
    "    def feature_vector(self, bytez):\n",
    "        return self.process_raw_features(self.raw_features(bytez))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4dced",
   "metadata": {},
   "source": [
    "##### features are extracted from the binary files saved in original folders, modified_folders, adv_folders and and saved in correspond folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract file name\n",
    "def extract_file_name(file_path):\n",
    "    return os.path.basename(file_path)\n",
    "\n",
    "# Function to extract features and save to CSV\n",
    "def extract_and_save_features(input_folder, output_csv, folder_name):\n",
    "    feature_extractor = PEFeatureExtractor()\n",
    "    vectorized_list = []\n",
    "    fname_list = []\n",
    "\n",
    "    binary_folder_name = folder_name + \"_binary_files\"\n",
    "    binary_folder_path = os.path.join(input_folder, binary_folder_name)\n",
    "\n",
    "    for i, f in enumerate(os.listdir(binary_folder_path)):\n",
    "        path = os.path.join(binary_folder_path, f)\n",
    "        with open(path, \"rb\") as file_handle:\n",
    "            bytez = file_handle.read()\n",
    "\n",
    "        vectorized = feature_extractor.feature_vector(bytez)\n",
    "        vectorized_list.append(vectorized)\n",
    "\n",
    "        f_name = extract_file_name(path)\n",
    "        fname_list.append(f_name)\n",
    "\n",
    "    header = []\n",
    "    for h in range(1, 257):\n",
    "        header.append(f\"hist_{h}\")\n",
    "    for b in range(257, 513):\n",
    "        header.append(f\"byte_{b}\")\n",
    "    for s in range(513, 617):\n",
    "        header.append(f\"string_{s}\")\n",
    "    for g in range(617, 627):\n",
    "        header.append(f\"gen_{g}\")\n",
    "    for h in range(627, 689):\n",
    "        header.append(f\"head_{h}\")\n",
    "    for sec in range(689, 944):\n",
    "        header.append(f\"section_{sec}\")\n",
    "    for imp in range(944, 2224):\n",
    "        header.append(f\"imports_{imp}\")\n",
    "    for exp in range(2224, 2352):\n",
    "        header.append(f\"exports_{exp}\")\n",
    "    for d in range(2352, 2382):\n",
    "        header.append(f\"directories_{d}\")\n",
    "    with open(output_csv, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for ve in vectorized_list:\n",
    "            writer.writerow(ve)\n",
    "\n",
    "    new_column_header = \"f_name\"\n",
    "    df = pd.read_csv(output_csv)\n",
    "    df[new_column_header] = fname_list\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# List of folders to process\n",
    "folders_to_process = [\"fold1\", \"fold2\", \"fold3\", \"modified_fold1\", \"modified_fold2\", \"modified_fold3\", \"adv_fold1\", \"adv_fold2\", \"adv_fold3\"]\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder_name in folders_to_process:\n",
    "    folder_path = os.path.join(output_folder, folder_name)\n",
    "    output_csv_path = os.path.join(folder_path, f\"{folder_name}_lief.csv\")\n",
    "\n",
    "    extract_and_save_features(folder_path, output_csv_path, folder_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405235d4",
   "metadata": {},
   "source": [
    "#### folds contains files name and labels copy from fold1, fold2, fold3 and paste into corresponding modified and adv_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name in folds_folders:\n",
    "    # Source paths\n",
    "    source_csv_path = os.path.join(output_folder, folder_name, f\"{folder_name}_data.csv\")\n",
    "\n",
    "    # Destination paths\n",
    "    modified_folder_path = os.path.join(output_folder, f\"modified_{folder_name}\")\n",
    "    adv_folder_path = os.path.join(output_folder, f\"adv_{folder_name}\")\n",
    "\n",
    "    # Move CSV file to modified folder\n",
    "    modified_csv_path = os.path.join(modified_folder_path, f\"{folder_name}_data.csv\")\n",
    "    shutil.copy2(source_csv_path, modified_csv_path)\n",
    "    print(f\"Moved {source_csv_path} to {modified_csv_path}\")\n",
    "\n",
    "    # Move CSV file to adv folder\n",
    "    adv_csv_path = os.path.join(adv_folder_path, f\"{folder_name}_data.csv\")\n",
    "    shutil.copy2(source_csv_path, adv_csv_path)\n",
    "    print(f\"Moved {source_csv_path} to {adv_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679c490",
   "metadata": {},
   "source": [
    "#### read labels from folds and assign to lief_features files in correspondoing modified_folders and adv_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa862ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Insertion of labels in folders\n",
    "def merge_and_save(file_labels, files_features, output_csv):\n",
    "    # Read fold_data CSV\n",
    "    labels_data = pd.read_csv(file_labels)\n",
    "\n",
    "    # Read input folder CSV\n",
    "    features_data = pd.read_csv(files_features)\n",
    "\n",
    "    # Merge based on 'f_name'\n",
    "    merged_df = pd.merge(features_data,labels_data[['file_names', 'labels']], left_on='f_name', right_on='file_names', how='inner')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged_df.drop(columns=['file_names', 'f_name'], inplace=True)\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [\n",
    "    ('fold1_data.csv', 'fold1_lief.csv', 'fold1_updated.csv'),\n",
    "    ('fold2_data.csv', 'fold2_lief.csv', 'fold2_updated.csv'),\n",
    "    ('fold3_data.csv', 'fold3_lief.csv', 'fold3_updated.csv'),\n",
    "    ('fold1_data.csv', 'modified_fold1_lief.csv', 'modified_fold1_updated.csv'),\n",
    "    ('fold2_data.csv', 'modified_fold2_lief.csv', 'modified_fold2_updated.csv'),\n",
    "    ('fold3_data.csv', 'modified_fold3_lief.csv', 'modified_fold3_updated.csv'),\n",
    "    ('fold1_data.csv', 'adv_fold1_lief.csv', 'AT_fold1.csv'),\n",
    "    ('fold2_data.csv', 'adv_fold2_lief.csv', 'AT_fold2.csv'),\n",
    "    ('fold3_data.csv', 'adv_fold3_lief.csv', 'AT_fold3.csv'),\n",
    "]\n",
    "\n",
    "# Iterate through each file pair and process\n",
    "for fold_data_csv, fold_csv, output_csv in files_to_process:\n",
    "    subfolder_name = os.path.basename(fold_csv).split('_lief')[0]\n",
    "    labels_file_path = os.path.join(output_folder, subfolder_name, fold_data_csv)\n",
    "    features_file_path = os.path.join(output_folder, subfolder_name, fold_csv)\n",
    "    output_csv_path = os.path.join(output_folder, subfolder_name, output_csv)\n",
    "    merge_and_save(labels_file_path, features_file_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d20ff",
   "metadata": {},
   "source": [
    "#### lief_features files from fold1, fold2, fol3 combined with leif_features files of adv_folders to create adversarial training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pairs to concatenate and save\n",
    "concatenation_pairs = [\n",
    "    {\"source_subfolder\": \"fold1\", \"source_file\": \"fold1_updated.csv\", \"dest_subfolder\": \"adv_fold1\", \"dest_file\": \"AT_fold1.csv\"},\n",
    "    {\"source_subfolder\": \"fold2\", \"source_file\": \"fold2_updated.csv\", \"dest_subfolder\": \"adv_fold2\", \"dest_file\": \"AT_fold2.csv\"},\n",
    "    {\"source_subfolder\": \"fold3\", \"source_file\": \"fold3_updated.csv\", \"dest_subfolder\": \"adv_fold3\", \"dest_file\": \"AT_fold3.csv\"},\n",
    "]\n",
    "\n",
    "for pair in concatenation_pairs:\n",
    "    # Get paths for source and destination files\n",
    "    source_path = os.path.join(output_folder, pair[\"source_subfolder\"], pair[\"source_file\"])\n",
    "    dest_path = os.path.join(output_folder, pair[\"dest_subfolder\"], pair[\"dest_file\"])\n",
    "\n",
    "    # Read source CSV file\n",
    "    source_df = pd.read_csv(source_path)\n",
    "\n",
    "    # Read destination CSV file\n",
    "    dest_df = pd.read_csv(dest_path)\n",
    "\n",
    "    # Concatenate source and destination DataFrames\n",
    "    concatenated_df = pd.concat([dest_df, source_df], axis=0, ignore_index=True)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file\n",
    "    output_csv_path = os.path.join(output_folder, pair[\"dest_subfolder\"], f\"{pair['dest_file'].replace('.csv', '_updated.csv')}\")\n",
    "    concatenated_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Concatenated and saved files in {pair['source_subfolder']} with {pair['dest_subfolder']} and saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a05c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f624933",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1_updated = pd.read_csv(os.path.join(output_folder,'fold1/fold1_updated.csv'))\n",
    "fold2_updated = pd.read_csv(os.path.join(output_folder, 'fold2/fold2_updated.csv'))\n",
    "fold3_updated = pd.read_csv(os.path.join(output_folder, 'fold3/fold3_updated.csv'))\n",
    "modified_fold1_updated = pd.read_csv(os.path.join(output_folder, 'modified_fold1/modified_fold1_updated.csv'))\n",
    "modified_fold2_updated = pd.read_csv(os.path.join(output_folder, 'modified_fold2/modified_fold2_updated.csv'))\n",
    "modified_fold3_updated = pd.read_csv(os.path.join(output_folder, 'modified_fold3/modified_fold3_updated.csv'))\n",
    "AT_fold1_updated = pd.read_csv(os.path.join(output_folder, 'adv_fold1/AT_fold1_updated.csv'))\n",
    "AT_fold2_updated = pd.read_csv(os.path.join(output_folder, 'adv_fold2/AT_fold2_updated.csv'))\n",
    "AT_fold3_updated = pd.read_csv(os.path.join(output_folder, 'adv_fold3/AT_fold3_updated.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854851a",
   "metadata": {},
   "source": [
    "### training and evaluating the model with Trial1, Trial2 and Trial3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of datasets for each trial\n",
    "trials = [\n",
    "    (['fold1_updated', 'fold2_updated'], 'fold3_updated', 'modified_fold3_updated', 'AT_fold3_updated'),\n",
    "    (['fold1_updated', 'fold3_updated'], 'fold2_updated', 'modified_fold2_updated', 'AT_fold2_updated'),\n",
    "    (['fold2_updated', 'fold3_updated'], 'fold1_updated', 'modified_fold1_updated', 'AT_fold1_updated'),\n",
    "    (['modified_fold1_updated', 'modified_fold2_updated'], 'fold3_updated', 'modified_fold3_updated', 'AT_fold3_updated'),\n",
    "    (['modified_fold1_updated', 'modified_fold3_updated'], 'fold2_updated', 'modified_fold2_updated', 'AT_fold2_updated'),\n",
    "    (['modified_fold2_updated', 'modified_fold3_updated'], 'fold1_updated', 'modified_fold1_updated', 'AT_fold1_updated'),\n",
    "    (['AT_fold1_updated', 'AT_fold2_updated'], 'fold3_updated', 'modified_fold3_updated', 'AT_fold3_updated'),\n",
    "    (['AT_fold1_updated', 'AT_fold3_updated'], 'fold2_updated', 'modified_fold2_updated', 'AT_fold2_updated'),\n",
    "    (['AT_fold2_updated', 'AT_fold3_updated'], 'fold1_updated', 'modified_fold1_updated', 'AT_fold1_updated'),\n",
    "]\n",
    "\n",
    "# Iterate through each trial\n",
    "for i, trial in enumerate(trials, start=1):\n",
    "    train_datasets, test_fold, test_modified_fold, test_AT_fold = trial\n",
    "\n",
    "    # Concatenate training datasets\n",
    "    training_data = pd.concat([globals()[dataset] for dataset in train_datasets])\n",
    "\n",
    "    # Extract features and labels for training\n",
    "    x_train = training_data.iloc[:, :-1]\n",
    "    y_train = training_data.iloc[:, -1]\n",
    "\n",
    "    # Extract features and labels for testing\n",
    "    x_test_fold = globals()[test_fold].iloc[:, :-1]\n",
    "    y_test_fold = globals()[test_fold].iloc[:, -1]\n",
    "\n",
    "    x_test_modified_fold = globals()[test_modified_fold].iloc[:, :-1]\n",
    "    y_test_modified_fold = globals()[test_modified_fold].iloc[:, -1]\n",
    "\n",
    "    x_test_AT_fold = globals()[test_AT_fold].iloc[:, :-1]\n",
    "    y_test_AT_fold = globals()[test_AT_fold].iloc[:, -1]\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.LGBMClassifier()\n",
    "    model = model.fit(x_train, y_train)\n",
    "    train_set_name = '+'.join(train_datasets)\n",
    "    model_filename = f\"trial_{i}_train_{train_set_name}_model.pkl\"\n",
    "    model_path = os.path.join(model_folder, model_filename)\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Evaluate performance on test sets\n",
    "    for test_set_name, x_test, y_test in zip(['fold', 'modified_fold', 'AT_fold'],\n",
    "                                            [x_test_fold, x_test_modified_fold, x_test_AT_fold],\n",
    "                                            [y_test_fold, y_test_modified_fold, y_test_AT_fold]):\n",
    "        # Predict on the test set\n",
    "        y_predicted = np.argmax(model.predict_proba(x_test), axis=1)\n",
    "\n",
    "        # Evaluate performance\n",
    "        confusion_mat = confusion_matrix(y_test, y_predicted)\n",
    "        accuracy = accuracy_score(y_test, y_predicted)\n",
    "        classification_rep = classification_report(y_test, y_predicted)\n",
    "\n",
    "        # Generate result filename\n",
    "        \n",
    "        result_filename = f\"trial_{i}_train_{train_set_name}_test_{test_set_name}\"\n",
    "\n",
    "        # Write results to file\n",
    "        result_path = os.path.join(results_folder, result_filename)\n",
    "        with open(result_path, \"w\") as text_file:\n",
    "            text_file.write(f'Accuracy: {accuracy}')\n",
    "            text_file.write(f'\\nClassification: {classification_rep}')\n",
    "            text_file.write(f'\\nConfusion: {confusion_mat}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
