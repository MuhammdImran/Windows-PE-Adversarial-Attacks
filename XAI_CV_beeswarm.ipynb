{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shap\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "import csv\n",
    "import shutil\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "xai = 'XAI_CV'\n",
    "base_directory = config.get(xai, 'output')\n",
    "output_folder = config.get(xai, 'output_folder')\n",
    "fold1_file_names = config.get(xai, 'fold1_data')\n",
    "fold2_file_names = config.get(xai, 'fold2_data')\n",
    "fold3_file_names = config.get(xai, 'fold3_data')\n",
    "O_M_f2f3 = config.get(xai, 'O_M_f2f3')\n",
    "AT_M_f2f3 = config.get(xai, 'AT_M_f2f3')\n",
    "O_M_f1f3 = config.get(xai, 'O_M_f1f3')\n",
    "AT_M_f1f3 = config.get(xai, 'AT_M_f1f3')\n",
    "O_M_f1f2 = config.get(xai, 'O_M_f1f2')\n",
    "AT_M_f1f2 = config.get(xai, 'AT_M_f1f2')\n",
    "models_directory = config.get(xai, 'models')\n",
    "\n",
    "fold1_file_names = pd.read_csv(fold1_file_names)\n",
    "fold2_file_names = pd.read_csv(fold2_file_names)\n",
    "fold3_file_names = pd.read_csv(fold3_file_names)\n",
    "\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.rmtree(output_folder)\n",
    "os.makedirs(output_folder)\n",
    "\n",
    "# Dictionaries to map folders to their respective models\n",
    "models_for_folders = {\n",
    "    \"fold1\": [\"trial_3_train_fold2_updated+fold3_updated_model\", \"trial_9_train_AT_fold2_updated+AT_fold3_updated_model\"],\n",
    "    \"adv_fold1\": [\"trial_3_train_fold2_updated+fold3_updated_model\", \"trial_9_train_AT_fold2_updated+AT_fold3_updated_model\"],\n",
    "    \"fold2\": [\"trial_2_train_fold1_updated+fold3_updated_model\", \"trial_8_train_AT_fold1_updated+AT_fold3_updated_model\"],\n",
    "    \"adv_fold2\": [\"trial_2_train_fold1_updated+fold3_updated_model\", \"trial_8_train_AT_fold1_updated+AT_fold3_updated_model\"],\n",
    "    \"fold3\": [\"trial_1_train_fold1_updated+fold2_updated_model\", \"trial_7_train_AT_fold1_updated+AT_fold2_updated_model\"],\n",
    "    \"adv_fold3\": [\"trial_1_train_fold1_updated+fold2_updated_model\", \"trial_7_train_AT_fold1_updated+AT_fold2_updated_model\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524139c9",
   "metadata": {},
   "source": [
    "#######extracting feature from PE binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/python\n",
    "''' Extracts some basic features from PE files. Many of the features\n",
    "implemented have been used in previously published works. For more information,\n",
    "check out the following resources:\n",
    "* Schultz, et al., 2001: http://128.59.14.66/sites/default/files/binaryeval-ieeesp01.pdf\n",
    "* Kolter and Maloof, 2006: http://www.jmlr.org/papers/volume7/kolter06a/kolter06a.pdf\n",
    "* Shafiq et al., 2009: https://www.researchgate.net/profile/Fauzan_Mirza/publication/242084613_A_Framework_for_Efficient_Mining_of_Structural_Information_to_Detect_Zero-Day_Malicious_Portable_Executables/links/0c96052e191668c3d5000000.pdf\n",
    "* Raman, 2012: http://2012.infosecsouthwest.com/files/speaker_materials/ISSW2012_Selecting_Features_to_Classify_Malware.pdf\n",
    "* Saxe and Berlin, 2015: https://arxiv.org/pdf/1508.03096.pdf\n",
    "It may be useful to do feature selection to reduce this set of features to a meaningful set\n",
    "for your modeling problem.\n",
    "'''\n",
    "\n",
    "import re\n",
    "import lief\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split('.')\n",
    "LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or ( int(LIEF_MAJOR)==0 and int(LIEF_MINOR) >= 10 )\n",
    "LIEF_HAS_SIGNATURE = int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 11)\n",
    "\n",
    "\n",
    "class FeatureType(object):\n",
    "    ''' Base class from which each feature type may inherit '''\n",
    "\n",
    "    name = ''\n",
    "    dim = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, self.dim)\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        ''' Generate a JSON-able representation of the file '''\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        ''' Generate a feature vector from the raw features '''\n",
    "        raise (NotImplementedError)\n",
    "\n",
    "    def feature_vector(self, bytez, lief_binary):\n",
    "        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n",
    "        if there are significant speedups to be gained from combining the two functions. '''\n",
    "        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n",
    "\n",
    "\n",
    "class ByteHistogram(FeatureType):\n",
    "    ''' Byte histogram (count + non-normalized) over the entire binary file '''\n",
    "\n",
    "    name = 'histogram'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n",
    "        return counts.tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class ByteEntropyHistogram(FeatureType):\n",
    "    ''' 2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).\n",
    "    This roughly approximates the joint probability of byte value and local entropy.\n",
    "    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.\n",
    "    '''\n",
    "\n",
    "    name = 'byteentropy'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self, step=1024, window=2048):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "\n",
    "    def _entropy_bin_counts(self, block):\n",
    "        # coarse histogram, 16 bytes per bin\n",
    "        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram\n",
    "        p = c.astype(np.float32) / self.window\n",
    "        wh = np.where(c)[0]\n",
    "        H = np.sum(-p[wh] * np.log2(\n",
    "            p[wh])) * 2  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)\n",
    "\n",
    "        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)\n",
    "        if Hbin == 16:  # handle entropy = 8.0 bits\n",
    "            Hbin = 15\n",
    "\n",
    "        return Hbin, c\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = np.zeros((16, 16), dtype=np.int)\n",
    "        a = np.frombuffer(bytez, dtype=np.uint8)\n",
    "        if a.shape[0] < self.window:\n",
    "            Hbin, c = self._entropy_bin_counts(a)\n",
    "            output[Hbin, :] += c\n",
    "        else:\n",
    "            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n",
    "            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n",
    "            strides = a.strides + (a.strides[-1],)\n",
    "            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n",
    "\n",
    "            # from the blocks, compute histogram\n",
    "            for block in blocks:\n",
    "                Hbin, c = self._entropy_bin_counts(block)\n",
    "                output[Hbin, :] += c\n",
    "\n",
    "        return output.flatten().tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class SectionInfo(FeatureType):\n",
    "    ''' Information about section names, sizes and entropy.  Uses hashing trick\n",
    "    to summarize all this section info into a feature vector.\n",
    "    '''\n",
    "\n",
    "    name = 'section'\n",
    "    dim = 5 + 50 + 50 + 50 + 50 + 50\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _properties(s):\n",
    "        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\"entry\": \"\", \"sections\": []}\n",
    "\n",
    "        # properties of entry point, or if invalid, the first executable section\n",
    "\n",
    "        try:\n",
    "            if int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 12):\n",
    "                section = lief_binary.section_from_rva(lief_binary.entrypoint - lief_binary.imagebase)\n",
    "                if section is None:\n",
    "                    raise lief.not_found\n",
    "                entry_section = section.name\n",
    "            else: # lief < 0.12\n",
    "                entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n",
    "        except lief.not_found:\n",
    "                # bad entry point, let's find the first executable section\n",
    "                entry_section = \"\"\n",
    "                for s in lief_binary.sections:\n",
    "                    if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n",
    "                        entry_section = s.name\n",
    "                        break\n",
    "\n",
    "        raw_obj = {\"entry\": entry_section}\n",
    "        raw_obj[\"sections\"] = [{\n",
    "            'name': s.name,\n",
    "            'size': s.size,\n",
    "            'entropy': s.entropy,\n",
    "            'vsize': s.virtual_size,\n",
    "            'props': self._properties(s)\n",
    "        } for s in lief_binary.sections]\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        sections = raw_obj['sections']\n",
    "        general = [\n",
    "            len(sections),  # total number of sections\n",
    "            # number of sections with zero size\n",
    "            sum(1 for s in sections if s['size'] == 0),\n",
    "            # number of sections with an empty name\n",
    "            sum(1 for s in sections if s['name'] == \"\"),\n",
    "            # number of RX\n",
    "            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
    "            # number of W\n",
    "            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
    "        ]\n",
    "        # gross characteristics of each section\n",
    "        section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "        section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
    "        section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([raw_obj['entry']]).toarray()[0]\n",
    "        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
    "        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "\n",
    "        return np.hstack([\n",
    "            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n",
    "            characteristics_hashed\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ImportsInfo(FeatureType):\n",
    "    ''' Information about imported libraries and functions from the\n",
    "    import address table.  Note that the total number of imported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'imports'\n",
    "    dim = 1280\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        imports = {}\n",
    "        if lief_binary is None:\n",
    "            return imports\n",
    "\n",
    "        for lib in lief_binary.imports:\n",
    "            if lib.name not in imports:\n",
    "                imports[lib.name] = []  # libraries can be duplicated in listing, extend instead of overwrite\n",
    "\n",
    "            # Clipping assumes there are diminishing returns on the discriminatory power of imported functions\n",
    "            #  beyond the first 10000 characters, and this will help limit the dataset size\n",
    "            for entry in lib.entries:\n",
    "                if entry.is_ordinal:\n",
    "                    imports[lib.name].append(\"ordinal\" + str(entry.ordinal))\n",
    "                else:\n",
    "                    imports[lib.name].append(entry.name[:10000])\n",
    "\n",
    "        return imports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        # unique libraries\n",
    "        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n",
    "\n",
    "        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n",
    "        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n",
    "        imports_hashed = FeatureHasher(1024, input_type=\"string\").transform([imports]).toarray()[0]\n",
    "\n",
    "        # Two separate elements: libraries (alone) and fully-qualified names of imported functions\n",
    "        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)\n",
    "\n",
    "\n",
    "class ExportsInfo(FeatureType):\n",
    "    ''' Information about exported functions. Note that the total number of exported\n",
    "    functions is contained in GeneralFileInfo.\n",
    "    '''\n",
    "\n",
    "    name = 'exports'\n",
    "    dim = 128\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return []\n",
    "\n",
    "        # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond\n",
    "        #  the first 10000 characters, and this will help limit the dataset size\n",
    "        if LIEF_EXPORT_OBJECT:\n",
    "            # export is an object with .name attribute (0.10.0 and later)\n",
    "            clipped_exports = [export.name[:10000] for export in lief_binary.exported_functions]\n",
    "        else:\n",
    "            # export is a string (LIEF 0.9.0 and earlier)\n",
    "            clipped_exports = [export[:10000] for export in lief_binary.exported_functions]\n",
    "\n",
    "\n",
    "        return clipped_exports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n",
    "        return exports_hashed.astype(np.float32)\n",
    "\n",
    "\n",
    "class GeneralFileInfo(FeatureType):\n",
    "    ''' General information about the file '''\n",
    "\n",
    "    name = 'general'\n",
    "    dim = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        if lief_binary is None:\n",
    "            return {\n",
    "                'size': len(bytez),\n",
    "                'vsize': 0,\n",
    "                'has_debug': 0,\n",
    "                'exports': 0,\n",
    "                'imports': 0,\n",
    "                'has_relocations': 0,\n",
    "                'has_resources': 0,\n",
    "                'has_signature': 0,\n",
    "                'has_tls': 0,\n",
    "                'symbols': 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'size': len(bytez),\n",
    "            'vsize': lief_binary.virtual_size,\n",
    "            'has_debug': int(lief_binary.has_debug),\n",
    "            'exports': len(lief_binary.exported_functions),\n",
    "            'imports': len(lief_binary.imported_functions),\n",
    "            'has_relocations': int(lief_binary.has_relocations),\n",
    "            'has_resources': int(lief_binary.has_resources),\n",
    "            'has_signature': int(lief_binary.has_signatures) if LIEF_HAS_SIGNATURE else int(lief_binary.has_signature),\n",
    "            'has_tls': int(lief_binary.has_tls),\n",
    "            'symbols': len(lief_binary.symbols),\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.asarray([\n",
    "            raw_obj['size'], raw_obj['vsize'], raw_obj['has_debug'], raw_obj['exports'], raw_obj['imports'],\n",
    "            raw_obj['has_relocations'], raw_obj['has_resources'], raw_obj['has_signature'], raw_obj['has_tls'],\n",
    "            raw_obj['symbols']\n",
    "        ],\n",
    "                          dtype=np.float32)\n",
    "\n",
    "\n",
    "class HeaderFileInfo(FeatureType):\n",
    "    ''' Machine, architecure, OS, linker and other information extracted from header '''\n",
    "\n",
    "    name = 'header'\n",
    "    dim = 62\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        raw_obj = {}\n",
    "        raw_obj['coff'] = {'timestamp': 0, 'machine': \"\", 'characteristics': []}\n",
    "        raw_obj['optional'] = {\n",
    "            'subsystem': \"\",\n",
    "            'dll_characteristics': [],\n",
    "            'magic': \"\",\n",
    "            'major_image_version': 0,\n",
    "            'minor_image_version': 0,\n",
    "            'major_linker_version': 0,\n",
    "            'minor_linker_version': 0,\n",
    "            'major_operating_system_version': 0,\n",
    "            'minor_operating_system_version': 0,\n",
    "            'major_subsystem_version': 0,\n",
    "            'minor_subsystem_version': 0,\n",
    "            'sizeof_code': 0,\n",
    "            'sizeof_headers': 0,\n",
    "            'sizeof_heap_commit': 0\n",
    "        }\n",
    "        if lief_binary is None:\n",
    "            return raw_obj\n",
    "\n",
    "        raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps\n",
    "        raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]\n",
    "        raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]\n",
    "        raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]\n",
    "        raw_obj['optional']['dll_characteristics'] = [\n",
    "            str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists\n",
    "        ]\n",
    "        raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]\n",
    "        raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version\n",
    "        raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version\n",
    "        raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version\n",
    "        raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version\n",
    "        raw_obj['optional'][\n",
    "            'major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version\n",
    "        raw_obj['optional'][\n",
    "            'minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version\n",
    "        raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version\n",
    "        raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version\n",
    "        raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code\n",
    "        raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers\n",
    "        raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.hstack([\n",
    "            raw_obj['coff']['timestamp'],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['coff']['machine']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['coff']['characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['subsystem']]]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['optional']['dll_characteristics']]).toarray()[0],\n",
    "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['magic']]]).toarray()[0],\n",
    "            raw_obj['optional']['major_image_version'],\n",
    "            raw_obj['optional']['minor_image_version'],\n",
    "            raw_obj['optional']['major_linker_version'],\n",
    "            raw_obj['optional']['minor_linker_version'],\n",
    "            raw_obj['optional']['major_operating_system_version'],\n",
    "            raw_obj['optional']['minor_operating_system_version'],\n",
    "            raw_obj['optional']['major_subsystem_version'],\n",
    "            raw_obj['optional']['minor_subsystem_version'],\n",
    "            raw_obj['optional']['sizeof_code'],\n",
    "            raw_obj['optional']['sizeof_headers'],\n",
    "            raw_obj['optional']['sizeof_heap_commit'],\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class StringExtractor(FeatureType):\n",
    "    ''' Extracts strings from raw byte stream '''\n",
    "\n",
    "    name = 'strings'\n",
    "    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # all consecutive runs of 0x20 - 0x7f that are 5+ characters\n",
    "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
    "        # occurances of the string 'C:\\'.  Not actually extracting the path\n",
    "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
    "        # occurances of http:// or https://.  Not actually extracting the URLs\n",
    "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
    "        # occurances of the string prefix HKEY_.  No actually extracting registry names\n",
    "        self._registry = re.compile(b'HKEY_')\n",
    "        # crude evidence of an MZ header (dropper?) somewhere in the byte stream\n",
    "        self._mz = re.compile(b'MZ')\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        allstrings = self._allstrings.findall(bytez)\n",
    "        if allstrings:\n",
    "            # statistics about strings:\n",
    "            string_lengths = [len(s) for s in allstrings]\n",
    "            avlength = sum(string_lengths) / len(string_lengths)\n",
    "            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
    "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
    "            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n",
    "            # distribution of characters in printable strings\n",
    "            csum = c.sum()\n",
    "            p = c.astype(np.float32) / csum\n",
    "            wh = np.where(c)[0]\n",
    "            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n",
    "        else:\n",
    "            avlength = 0\n",
    "            c = np.zeros((96,), dtype=np.float32)\n",
    "            H = 0\n",
    "            csum = 0\n",
    "\n",
    "        return {\n",
    "            'numstrings': len(allstrings),\n",
    "            'avlength': avlength,\n",
    "            'printabledist': c.tolist(),  # store non-normalized histogram\n",
    "            'printables': int(csum),\n",
    "            'entropy': float(H),\n",
    "            'paths': len(self._paths.findall(bytez)),\n",
    "            'urls': len(self._urls.findall(bytez)),\n",
    "            'registry': len(self._registry.findall(bytez)),\n",
    "            'MZ': len(self._mz.findall(bytez))\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n",
    "        return np.hstack([\n",
    "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
    "            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n",
    "            raw_obj['registry'], raw_obj['MZ']\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "\n",
    "class DataDirectories(FeatureType):\n",
    "    ''' Extracts size and virtual address of the first 15 data directories '''\n",
    "\n",
    "    name = 'datadirectories'\n",
    "    dim = 15 * 2\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self._name_order = [\n",
    "            \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n",
    "            \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n",
    "            \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n",
    "        ]\n",
    "\n",
    "    def raw_features(self, bytez, lief_binary):\n",
    "        output = []\n",
    "        if lief_binary is None:\n",
    "            return output\n",
    "\n",
    "        for data_directory in lief_binary.data_directories:\n",
    "            output.append({\n",
    "                \"name\": str(data_directory.type).replace(\"DATA_DIRECTORY.\", \"\"),\n",
    "                \"size\": data_directory.size,\n",
    "                \"virtual_address\": data_directory.rva\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        features = np.zeros(2 * len(self._name_order), dtype=np.float32)\n",
    "        for i in range(len(self._name_order)):\n",
    "            if i < len(raw_obj):\n",
    "                features[2 * i] = raw_obj[i][\"size\"]\n",
    "                features[2 * i + 1] = raw_obj[i][\"virtual_address\"]\n",
    "        return features\n",
    "\n",
    "\n",
    "class PEFeatureExtractor(object):\n",
    "    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n",
    "\n",
    "    def __init__(self, feature_version=2, print_feature_warning=True, features_file=''):\n",
    "        self.features = []\n",
    "        features = {\n",
    "                    'ByteHistogram': ByteHistogram(),\n",
    "                    'ByteEntropyHistogram': ByteEntropyHistogram(),\n",
    "                    'StringExtractor': StringExtractor(),\n",
    "                    'GeneralFileInfo': GeneralFileInfo(),\n",
    "                    'HeaderFileInfo': HeaderFileInfo(),\n",
    "                    'SectionInfo': SectionInfo(),\n",
    "                    'ImportsInfo': ImportsInfo(),\n",
    "                    'ExportsInfo': ExportsInfo()\n",
    "            }\n",
    "\n",
    "        if os.path.exists(features_file):\n",
    "            with open(features_file, encoding='utf8') as f:\n",
    "                x = json.load(f)\n",
    "                self.features = [features[feature] for feature in x['features'] if feature in features]\n",
    "        else:\n",
    "            self.features = list(features.values())\n",
    "\n",
    "        if feature_version == 1:\n",
    "            if not lief.__version__.startswith(\"0.8.3\"):\n",
    "                if print_feature_warning:\n",
    "                    print(f\"WARNING: EMBER feature version 1 were computed using lief version 0.8.3-18d5b75\")\n",
    "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
    "                    print(f\"WARNING:   in the feature calculations.\")\n",
    "        elif feature_version == 2:\n",
    "            self.features.append(DataDirectories())\n",
    "            if not lief.__version__.startswith(\"0.9.0\"):\n",
    "                if print_feature_warning:\n",
    "                    print(f\"WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\")\n",
    "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
    "                    print(f\"WARNING:   in the feature calculations.\")\n",
    "        else:\n",
    "            raise Exception(f\"EMBER feature version must be 1 or 2. Not {feature_version}\")\n",
    "        self.dim = sum([fe.dim for fe in self.features])\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n",
    "                       RuntimeError)\n",
    "        try:\n",
    "            lief_binary = lief.PE.parse(list(bytez))\n",
    "        except lief_errors as e:\n",
    "            #print(\"lief error: \", str(e))\n",
    "            lief_binary = None\n",
    "        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n",
    "            raise\n",
    "\n",
    "        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n",
    "        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n",
    "        return features\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n",
    "        return np.hstack(feature_vectors).astype(np.float32)\n",
    "\n",
    "    def feature_vector(self, bytez):\n",
    "        return self.process_raw_features(self.raw_features(bytez))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = PEFeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30560284",
   "metadata": {},
   "source": [
    "#######find samples from extended dataset that are predicted as FN by models trained on original folds and predicted as TP by models trained on extended fold. Computed the SHAp values for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def features_extraction(file_path):\n",
    "    # Load the LightGBM model\n",
    "    with open(file_path, \"rb\") as file_handle:\n",
    "        bytez= file_handle.read()\n",
    "    features = feature_extractor.feature_vector(bytez)\n",
    "    return features\n",
    "\n",
    "def load_and_evaluate_model(model_path, features):\n",
    "    model = joblib.load(model_path)\n",
    "    result = model.predict(features)\n",
    "    return result\n",
    "\n",
    "def header_values():\n",
    "    for h in range(1, 257):\n",
    "        header_columns.append(f\"hist_{h}\")\n",
    "    for b in range(257, 513):\n",
    "        header_columns.append(f\"byte_{b}\")\n",
    "    for s in range(513, 617):\n",
    "        header_columns.append(f\"string_{s}\")\n",
    "    for g in range(617, 627):\n",
    "        header_columns.append(f\"gen_{g}\")\n",
    "    for h in range(627, 689):\n",
    "        header_columns.append(f\"head_{h}\")\n",
    "    for sec in range(689, 944):\n",
    "        header_columns.append(f\"section_{sec}\")\n",
    "    for imp in range(944, 2224):\n",
    "        header_columns.append(f\"imports_{imp}\")\n",
    "    for exp in range(2224, 2352):\n",
    "        header_columns.append(f\"exports_{exp}\")\n",
    "    for d in range(2352, 2382):\n",
    "        header_columns.append(f\"directories_{d}\")\n",
    "\n",
    "header_columns = []\n",
    "fold1_shap_values = []\n",
    "fold2_shap_values = []\n",
    "fold3_shap_values = []\n",
    "Mfold1 = 0\n",
    "Mfold2 = 0\n",
    "Mfold3 = 0\n",
    "fold1_features = []\n",
    "fold2_features = []\n",
    "fold3_features = []\n",
    "#concatenated csv files and created dictionary that contains files names and corresponding labels from extended dataset\n",
    "combined_files = pd.concat([fold1_file_names, fold2_file_names, fold3_file_names])\n",
    "file_label_dict = pd.Series(combined_files.labels.values, index=combined_files.file_names).to_dict()\n",
    "# Loop through each main folder and corresponding models \n",
    "for main_folder, model_names in models_for_folders.items():\n",
    "    main_folder_path = os.path.join(base_directory, main_folder)\n",
    "    fold_features = []\n",
    "    sub_folder = main_folder + \"_binary_files\"\n",
    "    sub_folder_path = os.path.join(main_folder_path, sub_folder)\n",
    "    for file_name in os.listdir(sub_folder_path):\n",
    "        file_path = os.path.join(sub_folder_path, file_name)\n",
    "        print(\"file_path\", file_path)\n",
    "        f_label = file_label_dict[file_name]\n",
    "        print(\"f_label\", f_label)\n",
    "        if f_label == 1:\n",
    "            features = features_extraction(file_path)\n",
    "            new_features= features.reshape(1, -1)\n",
    "            model1_path = os.path.join(models_directory, model_names[0] + '.pkl')\n",
    "            result1 = load_and_evaluate_model(model1_path, new_features)\n",
    "            print(\"result1\", result1)\n",
    "            model2_path = os.path.join(models_directory, model_names[1] + '.pkl')\n",
    "            result2 = load_and_evaluate_model(model2_path, new_features)\n",
    "            print(\"result2\", result2)\n",
    "            # Check the specific condition for the results and label\n",
    "            if (result1 == 0 and f_label == 1) and (result2 == 1 and f_label == 1):\n",
    "                if main_folder == \"fold1\" or main_folder == \"adv_fold1\":\n",
    "                    fold1_features.append(features)\n",
    "                    Mfold1= Mfold1+1\n",
    "                if main_folder == \"fold2\" or main_folder == \"adv_fold2\":\n",
    "                    fold2_features.append(features)\n",
    "                    Mfold2= Mfold2+1   \n",
    "                if main_folder == \"fold3\" or main_folder == \"adv_fold3\":\n",
    "                    fold3_features.append(features)\n",
    "                    Mfold3= Mfold3+1\n",
    "            \n",
    "# Compute SHAP values for fold1+adv_fold1\n",
    "M_Original_F = joblib.load(O_M_f2f3)\n",
    "explainer = shap.TreeExplainer(M_Original_F )\n",
    "M_f1_shap_values = explainer.shap_values(np.array(fold1_features))\n",
    "M_AT_F = joblib.load(AT_M_f2f3)\n",
    "explainer = shap.TreeExplainer(M_AT_F)\n",
    "M_AT_f1_shap_values = explainer.shap_values(np.array(fold1_features))\n",
    "# Compute SHAP values for fold2+adv_fold2       \n",
    "M_Original_F = joblib.load(O_M_f1f3)\n",
    "explainer = shap.TreeExplainer(M_Original_F )\n",
    "M_f2_shap_values = explainer.shap_values(np.array(fold2_features))\n",
    "M_AT_F = joblib.load(AT_M_f1f3)\n",
    "explainer = shap.TreeExplainer(M_AT_F)\n",
    "M_AT_f2_shap_values = explainer.shap_values(np.array(fold2_features))\n",
    "# Compute SHAP values for fold3+adv_fold3 \n",
    "M_Original_F = joblib.load(O_M_f1f2)\n",
    "explainer = shap.TreeExplainer(M_Original_F )\n",
    "M_f3_shap_values = explainer.shap_values(np.array(fold3_features))\n",
    "M_AT_F = joblib.load(AT_M_f1f2)\n",
    "explainer = shap.TreeExplainer(M_AT_F)\n",
    "M_AT_f3_shap_values = explainer.shap_values(np.array(fold3_features))\n",
    "\n",
    "#created the combined original tensor by concatenating all originl folds tensor and saved\n",
    "M_original_tensor = np.concatenate([M_f1_shap_values[1], M_f2_shap_values[1], M_f3_shap_values[1]], axis=0)\n",
    "tensorfile = \"M_original_tensor.npy\"\n",
    "tensor_path = os.path.join(output_folder, tensorfile)\n",
    "np.save(tensor_path, M_original_tensor)\n",
    "#created the combined extended tensor by concatenating all extended folds tensor and saved\n",
    "M_AT_tensor = np.concatenate([M_AT_f1_shap_values[1], M_AT_f2_shap_values[1], M_AT_f3_shap_values[1]], axis=0)\n",
    "tensorfile = \"M_AT_tensor.npy\"\n",
    "tensor_path = os.path.join(output_folder, tensorfile)\n",
    "np.save(tensor_path, M_AT_tensor)\n",
    "#call header values function to extract header column names\n",
    "header_values()\n",
    "#Combined all folds features files\n",
    "all_features = fold1_features + fold2_features + fold3_features\n",
    "concatenated_features = pd.DataFrame(all_features, columns=header_columns)\n",
    "features_file = \"features.csv\"\n",
    "features_file_path = os.path.join(output_folder, features_file)\n",
    "concatenated_features.to_csv(features_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3734cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Plotted shap for original_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_positive_class_shap_values = M_original_tensor\n",
    "feature_names = concatenated_features.columns.tolist()\n",
    "# Computed the mean SHAP values for each feature across all samples.\n",
    "mean_shap_values = np.mean(M_positive_class_shap_values, axis=0)\n",
    "# indices of the top 20 features based on the mean SHAP values.\n",
    "top_20_features_indices = np.argsort(mean_shap_values)[-20:][::-1]\n",
    "# Extracted names of the top 20 features.\n",
    "top_20_feature_names = [feature_names[i] for i in top_20_features_indices]\n",
    "for name, value in zip(top_20_feature_names, mean_shap_values[top_20_features_indices]):\n",
    "    print(f\"Feature: {name}, Mean SHAP value: {value}\")\n",
    "# 4. Filter out the SHAP values for only the top 20 features.\n",
    "top_20_shap_values = M_positive_class_shap_values[:, top_20_features_indices]\n",
    "desired_width_px = 1000\n",
    "desired_height_px = 800\n",
    "\n",
    "# Set the DPI\n",
    "dpi = 200  # Setting a DPI of 100 for example\n",
    "\n",
    "# Convert the desired dimensions to inches\n",
    "fig_width_in = desired_width_px / dpi\n",
    "fig_height_in = desired_height_px / dpi\n",
    "\n",
    "# Create a figure object with the desired size in inches\n",
    "fig, ax = plt.subplots(figsize=(fig_width_in, fig_height_in), dpi=dpi)\n",
    "# 5. Create an Explanation for the top 20 features.\n",
    "shap_values_top_20 = shap.Explanation(values=top_20_shap_values, \n",
    "                                      data=concatenated_features.iloc[:, top_20_features_indices].values, \n",
    "                                      feature_names=top_20_feature_names)\n",
    "# 6. beeswarm plot for the top 20 features.\n",
    "shap.plots.beeswarm(shap_values_top_20, max_display=20)\n",
    "shap_name = 'shap_on_original_tensor_plot.png'\n",
    "shap_path = os.path.join(output_folder, shap_name)\n",
    "# Save the figure with the set DPI\n",
    "fig.savefig(shap_path, bbox_inches='tight', facecolor='w', edgecolor='w', dpi=dpi)\n",
    "\n",
    "# Close the figure after saving to prevent memory issues\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7781b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Plotted shap for Extended_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38006d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_positive_class_shap_values = M_AT_tensor\n",
    "# Computed the mean SHAP values for each feature across all samples.\n",
    "mean_shap_values = np.mean(AT_positive_class_shap_values, axis=0)\n",
    "# indices of the top 20 features based on the mean SHAP values.\n",
    "top_20_features_indices = np.argsort(mean_shap_values)[-20:][::-1]\n",
    "# Extracted names of the top 20 features.\n",
    "top_20_feature_names = [feature_names[i] for i in top_20_features_indices]\n",
    "for name, value in zip(top_20_feature_names, mean_shap_values[top_20_features_indices]):\n",
    "    print(f\"Feature: {name}, Mean SHAP value: {value}\")\n",
    "# 4. Filter out the SHAP values for only the top 20 features.\n",
    "top_20_shap_values = AT_positive_class_shap_values[:, top_20_features_indices]\n",
    "dpi = 100  # Fixed DPI\n",
    "\n",
    "# Calculate the figure size in inches\n",
    "fig_width_in = 1000 / dpi  # Width in inches\n",
    "fig_height_in = 800 / dpi  # Height in inches\n",
    "\n",
    "# Create a figure with the calculated size\n",
    "fig, ax = plt.subplots(figsize=(fig_width_in, fig_height_in), dpi=dpi)\n",
    "\n",
    "# 5. Create an Explanation for the top 20 features.\n",
    "shap_values_top_20 = shap.Explanation(values=top_20_shap_values, \n",
    "                                      data=concatenated_features.iloc[:, top_20_features_indices].values, \n",
    "                                      feature_names=top_20_feature_names)\n",
    "# 6. beeswarm plot for the top 20 features.\n",
    "shap.plots.beeswarm(shap_values_top_20, max_display=20)\n",
    "# Define the output path\n",
    "shap_name = 'shap_on_AT_tensor_plot.png'\n",
    "shap_path = os.path.join(output_folder, shap_name)\n",
    "\n",
    "fig.savefig(shap_path, bbox_inches='tight', facecolor='w', edgecolor='w', dpi=dpi)\n",
    "\n",
    "# Close the figure\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
